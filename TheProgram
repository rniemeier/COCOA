##Computational Corpus Analysis
##Gets file from user, preprocesses it and output general data,
##then gets input from user to determine what analyses to run

import easygui
import nltk
import nltk.data
from nltk.collocations import *
import re ##regex stuff -> idk if using at all
import string
from collections import OrderedDict
import numpy as np
import matplotlib.pyplot as plt
from scipy import special
from datetime import datetime
import os.path as op


postags = {
    "CC": "conjunction, coordinating",
    "CD": "numeral, cardinal",
    "DT": "determiner",
    "EX": "existential there",
    "FW": "foreign word",
    "IN": "preposition or conjunction, subordinating",
    "JJ": "adjective or numeral, ordinal",
    "JJR": "adjective, comparative",
    "JJS": "adjective, superlative",
    "LS": "list item marker",
    "MD": "modal auxiliary",
    "NN": "noun, common, singular or mass",
    "NNP": "noun, proper, singular",
    "NNPS": "noun, proper, plural",
    "NNS": "noun, common, plural",
    "PDT": "pre-determiner",
    "POS": "genitive marker",
    "PRP": "pronoun, personal",
    "PRP$": "pronoun, possessive",
    "RB": "adverb",
    "RBR": "adverb, comparative",
    "RBS": "adverb, superlative",
    "RP": "particle",
    "SYM": "symbol",
    "TO": "\'to\' as preposition or infinitive marker",
    "UH": "interjection",
    "VB": "verb, base form",
    "VBD": "verb, past tense",
    "VBG": "verb, present participle or gerund",
    "VBN": "verb, past participle",
    "VBP": "verb, present tense, not 3rd person singular",
    "VBZ": "verb, present tense, 3rd person singular",
    "WDT": "WH-determiner",
    "WP": "WH-pronoun",
    "WP$": "WH-pronoun, possessive",
    "WRB": "WH-adverb"
    }



nountags = {
    "NN": "noun, common, singular or mass",
    "NNP": "noun, proper, singular",
    "NNPS": "noun, proper, plural",
    "NNS": "noun, common, plural"
    }

WHtags = {
    "WDT": "WH-determiner",
    "WP": "WH-pronoun",
    "WP$": "WH-pronoun, possessive",
    "WRB": "WH-adverb"
    }

advtags = {
    "RB": "adverb",
    "RBR": "adverb, comparative",
    "RBS": "adverb, superlative",
    "WRB": "WH-adverb"
    }

adjtags = {
    "JJ": "adjective or numeral, ordinal",
    "JJR": "adjective, comparative",
    "JJS": "adjective, superlative"
    }


verbtags = {
    "VB": "verb, base form",
    "VBD": "verb, past tense",
    "VBG": "verb, present participle or gerund",
    "VBN": "verb, past participle",
    "VBP": "verb, present tense, not 3rd person singular",
    "VBZ": "verb, present tense, 3rd person singular",
    "MD": "modal auxiliary"
    }
pronountags = {
    "WP": "WH-pronoun",
    "WP$": "WH-pronoun, possessive",
    "PRP": "pronoun, personal",
    "PRP$": "pronoun, possessive",
    "EX": "existential there"
    }

prepconjtags = {
    "CC": "conjunction, coordinating",
    "IN": "preposition or conjunction, subordinating",
    "TO": "\'to\' as preposition or infinitive marker"
    }

dettags = {
    "CD": "numeral, cardinal",
    "DT": "determiner",
    "LS": "list item marker",
    "PDT": "pre-determiner",
    "WDT": "WH-determiner"
    }

othertags = {
    "FW": "foreign word",
    "SYM": "symbol",
    "UH": "interjection",
    "POS": "genitive marker",
    "RP": "particle"
    }


def Preprocessor(sentences):
    #First, get rid of the stuff we don't want
    chapter = 0
    frequency = {}
    sorted_frequency = {}
    wordCount = 0
    individualWordCount = 0
    returnarray = []
    numOfSentences = 0
    for s in sentences:
        numOfSentences = numOfSentences + 1
        if("chapter" in s.lower()):
            chapter = chapter + 1
            remove = "Chapter " + str(chapter)
            s = s.replace(remove, "") #Remove chapter number
        s = s.replace('\n', ' ')  #Sentences shouldn't have
        s = s.replace('—', ' ')   #weird newlines or dashes. This also
                                  #makes our data prettier
        #Find the word frequency
        for word in s.split():
            wordCount = wordCount + 1;
            word = word.translate(str.maketrans('', '', string.punctuation))
            word = word.replace('“', '')
            word = word.replace('”', '')
            word = word.lower()
            if word in frequency: #word already seen
                frequency[word] = frequency[word] + 1
            else: #word hasn't been seen yet
                frequency[word] = 1
                individualWordCount = individualWordCount + 1
    returnarray.append("\n\nNumber of chapters: \t" + str(chapter) + "\n")
    returnarray.append("Number of tokens: \t" + str(wordCount) + "\n")
    returnarray.append("Number of token types: \t" + str(individualWordCount) + "\n")
    returnarray.append("Average Length of Sentences: \t" + str(wordCount/numOfSentences) + "\n\n\n")
                       
    #Ordered dictionary          
    frequencyGtoL = OrderedDict(reversed(sorted(frequency.items(), key=lambda x: x[1])))
    frequencyLtoG = OrderedDict((sorted(frequency.items(), key=lambda x: x[1])))
    top1000 = {k: frequencyGtoL[k] for k in list(frequencyGtoL)[:1000]}
    top100 = {k: frequencyGtoL[k] for k in list(frequencyGtoL)[:100]}
    low100 = {k: frequencyLtoG[k] for k in list(frequencyLtoG)[:100]}
    temp = "Hundred most common words: \n"
    i = 1;
    for key, value in top100.items():
        temp += str(i) + ".\t" + str(key) + "\t\t" + str(value) + "\n"
        i = i + 1
    temp += "\n\n"
    returnarray.append(temp)
    temp = "Hundred least common words: \n"
    i = 1;
    for key, value in low100.items():
        temp += str(i) + ".\t" + str(key) + "\t\t" + str(value) + "\n"
        i = i + 1
    temp += "\n\n"
    returnarray.append(temp)

    #keep this at end of Preprocesser
    return sentences, returnarray, top1000
 

def TermSearch(sentences):
    """Performs a lexical analysis on the file parameter)"""
    search = easygui.enterbox(msg="Enter your search term.", title="Search")
    output = ""
    while(search != None):
        results = []
        if(search != None):
            for s in sentences:
               if(search.lower() in s.lower()):
                    #results.append(s[((s.lower()).index(search.lower()) - 50):((s.lower()).index(search.lower()) + 50)])
                    results.append(s) #to do: iterate through file and compare each word, if the word is there get the index of that word and get the 50 chars on wither side of it. 
        output += "\n~~~~~The term _" + search + "_ appeared " + str(len(results)) + " time(s).~~~~~\n"
        #for r in results: #uncomment to get actual data
         #   output += r + "\n\n"
        search = easygui.enterbox(msg="Enter your search term.", title="Search")
    return output

def groupSearch(sentences):
    msg = "Enter the name of your group, then your search terms separated by commas.";
    title = "Group Search";
    fieldNames = ["Group Name", "Group Entries"]
    search = []
    search = easygui.multenterbox(msg, title, fieldNames)
    output = ""
    groupTotal = 0
    while(search != None):
        searcharray = search[1].split(",")
        if(searcharray != None):
            output += "~~~~~For the group " + search[0] + ":"
            for term in searcharray:
                results = []
                term = term.strip()
                for sentence in sentences:
                    if(term.lower() in sentence.lower()):
                        results.append("\"" + sentence + "\"\n")
                output += "\n~~~~~The term _" + term + "_ appeared " + str(len(results)) + " time(s).~~~~~\n"
                groupTotal += len(results)
                #for r in results: #uncomment to get actual data
                 #   output += r + "\n\n"
        output += "Group " + search[0] + " total appearances is: " + str(groupTotal)
        search = []
        search = easygui.multenterbox(msg, title, fieldNames)
        groupTotal = 0
    return output

def doubleWordSearch(sentences):
    search = easygui.enterbox(msg="Enter two seach terms", title="Search")
    output = ""
    searcharray = search.split(",")
    while(search != None):
        results = []
        chapter = 0
        if(search != None):
            for s in sentences:
                if(searcharray[0].lower() in s.lower() and searcharray[1].lower() in s.lower()):
                    results.append(s)
        output += "\n~~~~~The terms _" + searcharray[0] + "_ and _" + searcharray[1]  + "_ appeared in the same sentence " + str(len(results)) + " time(s).~~~~~\n"
        #for r in results: #uncomment to return actual data
        #    output += r + "\n\n"
        search = easygui.enterbox(msg="Enter two search terms.", title="Search")
    return output

def ZipfDistribution(filename, top1000):
    plt.plot(list(top1000.values()))
    plt.ylabel("Frequency")
    plt.xlabel("Tokens")
    plt.title("Zipf Plot of " + filename)
    plt.savefig("CompletedAnalyses\\" + filename + 'ZipfPlot.png')
    plt.clf() #comment this out and run multiple times to get overlaid zipf

def posAnalysis(sentences, filename):
    arrayOfPOS = [] #hold the tagged pos
    for s in sentences:
        words = nltk.word_tokenize(s)
        arrayOfPOS.append(nltk.pos_tag(words))

    arrayOfSpeechParts = {} #dictionary that hold that POS and frequency of occurances
    for sentence in arrayOfPOS:
        for word in sentence:
            if(word[1] in postags.keys()):
                if(word[1] not in arrayOfSpeechParts):
                    arrayOfSpeechParts[word[1]] = 1
                else:
                    arrayOfSpeechParts[word[1]] = arrayOfSpeechParts[word[1]] + 1

    arrayOfSpeechParts = OrderedDict((sorted(arrayOfSpeechParts.items(), key=lambda x: x[1])))

    adjPOS = {}
    nounPOS = {}
    WHPOS = {}
    advPOS = {}
    verbPOS = {}
    pnounPOS = {}
    pcPOS = {}
    detPOS = {}
    otherPOS = {}

    #Initialize all the tags to zero in a the order that they come in
    for a in adjtags:
        adjPOS[postags[a]] = 0
    for n in nountags:
        nounPOS[postags[n]] = 0
    for p in pronountags:
        pnounPOS[postags[p]] = 0
    for v in verbtags:
        verbPOS[postags[v]] = 0
    for w in WHtags:
        WHPOS[postags[w]] = 0
    for ad in advtags:
        advPOS[postags[ad]] = 0
    for pc in prepconjtags:
        pcPOS[postags[pc]] = 0
    for d in dettags:
        detPOS[postags[d]] = 0
    for o in othertags:
        otherPOS[postags[o]] = 0

    #Update the dictionary entry with the actual number. This keeps the colors consistant
    for pos in arrayOfSpeechParts:
        if (pos in verbtags):
            verbPOS[postags[pos]] = arrayOfSpeechParts[pos]
        elif(pos in WHtags):
            WHPOS[postags[pos]] = arrayOfSpeechParts[pos]
        elif(pos in advtags):
            advPOS[postags[pos]] = arrayOfSpeechParts[pos]
        elif(pos in adjtags):
            adjPOS[postags[pos]] = arrayOfSpeechParts[pos]
        elif(pos in pronountags):
            pnounPOS[postags[pos]] = arrayOfSpeechParts[pos]
        elif(pos in nountags):
            nounPOS[postags[pos]] = arrayOfSpeechParts[pos]
        elif(pos in prepconjtags):
            pcPOS[postags[pos]] = arrayOfSpeechParts[pos]
        elif(pos in dettags):
            detPOS[postags[pos]] = arrayOfSpeechParts[pos]
        else:
            otherPOS[postags[pos]] = arrayOfSpeechParts[pos]

#### Uncomment this region to produce pie charts of the data
##    posNames = ["Noun", "Pronoun", "Adjective", "Adverb", "Verb", "Wh", "Preposition/Conjuntion", "Determiner", "Other"]
##    posVals = [sum(nounPOS.values()), sum(pnounPOS.values()), sum(adjPOS.values()), sum(adjPOS.values()), sum(verbPOS.values()), sum(WHPOS.values()), sum(pcPOS.values()),sum(detPOS.values()), sum(otherPOS.values())]       
##    fig1, total = plt.subplots()
##    total.pie(posVals, explode=None, labels=posNames, autopct='%1.1f%%', shadow=False, startangle=90)
##    total.axis('equal')
##    plt.title("Total Parts of Speech Breakdown of " + filename)
##    plt.savefig("CompletedAnalyses\\" + filename + "TotalPartsOfSpeechPlot.png")
##    plt.clf()
##
##    fig1, totalWH = plt.subplots()
##    totalWH.pie(WHPOS.values(), explode=None, labels=WHPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalWH.axis('equal')
##    plt.title("WH- Usage Breakdown " + filename)
##    plt.savefig("CompletedAnalyses\\" + filename + "WHPlot.png")
##    plt.clf()
##    
##    fig1, totalAdj = plt.subplots()
##    totalAdj.pie(adjPOS.values(), explode=None, labels=adjPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalAdj.axis('equal')
##    plt.title(filename + " Adjective Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename + "AdjectivePlot.png")
##    plt.clf()
##    fig1.clf()
##    
##    fig1, totalAdverb = plt.subplots()
##    totalAdverb.pie(advPOS.values(), explode=None, labels=advPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalAdverb.axis('equal')
##    plt.title(filename + " Adverb Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename + "AdverbPlot.png")
##    plt.clf()
##
##    fig1, totalVerb = plt.subplots()
##    totalVerb.pie(verbPOS.values(), explode=None, labels=verbPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalVerb.axis('equal')
##    plt.title(filename + " Verb Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename + "VerbPlot.png")
##    plt.clf()
##
##    fig1, totalNoun = plt.subplots()
##    totalNoun.pie(nounPOS.values(), explode=None, labels=nounPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalNoun.axis('equal')
##    plt.title(filename + " Noun Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename + "NounPlot.png")
##    plt.clf()
##
##    fig1, totalPronoun = plt.subplots()
##    totalPronoun.pie(pnounPOS.values(), explode=None, labels=pnounPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalPronoun.axis('equal')
##    plt.title(filename + " Pronoun Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename + "PronounPlot.png")
##    plt.clf()
##
##    fig1, totalPrepConj = plt.subplots()
##    totalPrepConj.pie(pcPOS.values(), explode=None, labels=pcPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalPrepConj.axis('equal')
##    plt.title(filename + " Preposition/Conjunction Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename + "PrepConjPlot.png")
##    plt.clf()
##    
##    fig1, totalDet = plt.subplots()
##    totalDet.pie(detPOS.values(), explode=None, labels=detPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalDet.axis('equal')
##    plt.title(filename + " Determiner Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename + "DetPlot.png")
##    plt.clf()
##
##    fig1, totalOther = plt.subplots()
##    totalOther.pie(otherPOS.values(), explode=None, labels=otherPOS.keys(), autopct='%1.1f%%', shadow=False, startangle=90)
##    totalOther.axis('equal')
##    plt.title(filename + " Other Parts of Speech Breakdown")
##    plt.savefig("CompletedAnalyses\\" + filename+ "OtherPlot.png")
##    plt.clf()
##    plt.close('all')

    percent = ""

    percent += "WH\n"
    for key in WHPOS.keys():
        percent += "key: " + key + "\t value: " + str(WHPOS[key]) + "\t percent: " + str(100 * (WHPOS[key]/(sum(WHPOS.values())))) + "%\n"
    percent += "ADJ\n"
    for key in adjPOS.keys():
        percent += "key: " + key + "\t value: " + str(adjPOS[key]) + "\t percent: " + str(100 * (adjPOS[key]/(sum(adjPOS.values())))) + "%\n"
    percent += "ADV\n"
    for key in advPOS.keys():
        percent += "key: " + key + "\t value: " + str(advPOS[key]) + "\t percent: " + str(100 * (advPOS[key]/(sum(advPOS.values())))) + "%\n"
    percent += "VERB\n"
    for key in verbPOS.keys():
        percent += "key: " + key + "\t value: " + str(verbPOS[key]) + "\t percent: " + str(100 * (verbPOS[key]/(sum(verbPOS.values())))) + "%\n"
    percent += "NOUN\n"
    for key in nounPOS.keys():
        percent += "key: " + key + "\t value: " + str(nounPOS[key]) + "\t percent: " + str(100 * (nounPOS[key]/(sum(nounPOS.values())))) + "%\n"
    percent += "PNOUN\n" 
    for key in pnounPOS.keys():
        percent += "key: " + key + "\t value: " + str(pnounPOS[key]) + "\t percent: " + str(100 * (pnounPOS[key]/(sum(pnounPOS.values())))) + "%\n"
    percent += "PrepConj\n"
    for key in pcPOS.keys():
        percent += "key: " + key + "\t value: " + str(pcPOS[key]) + "\t percent: " + str(100 * (pcPOS[key]/(sum(pcPOS.values())))) + "%\n"
    percent += "DET\n"
    for key in detPOS.keys():
        percent += "key: " + key + "\t value: " + str(detPOS[key]) + "\t percent: " + str(100 * (detPOS[key]/(sum(detPOS.values())))) + "%\n"
    percent += "OTHER\n"
    for key in otherPOS.keys():
        percent += "key: " + key + "\t value: " + str(otherPOS[key]) + "\t percent: " + str(100 * (otherPOS[key]/(sum(otherPOS.values())))) + "%\n"
    return percent


    
##def collocation(aFile):
##    choices = ["bigram", "trigram", "quadgram"]
##    search = easygui.multchoicebox("Make a selection.", "Collocation", choices)
##    tokens = nltk.wordpunct_tokenize(aFile.read())
##    for s in search:
##        if (s == choices[0]):
##            #bigram
##            bigram_measures = nltk.collocations.BigramAssocMeasures()
##            word_fd = nltk.FreqDist(tokens)
##            bigram_fd = nltk.FreqDist(nltk.bigrams(tokens))
##            finder = BigramCollocationFinder(word_fd, bigram_fd)
##            scored = finder.score_ngrams(bigram_measures.raw_freq)
##            print(sorted(bigram for bigram, score in scored))
##            
##        if (s == choices[1]):
##            #trigram
##            print ("hello")
##        if(s == choices[2]):
##            #quadgram
##            print ("oops")
    
def main():
    file = easygui.fileopenbox()
    if(file != None):
        thisFile = open(file, 'r+', encoding="utf8") #open file
        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
        sentences = nltk.sent_tokenize(thisFile.read())
        words = nltk.word_tokenize(thisFile.read())
        sentences, quickfacts, top1000 = Preprocessor(sentences)
        filebase = op.basename(file)
        cleanfilename, file_exten = op.splitext(filebase)
        newFileName = "CompletedAnalyses\\AnalysisOf" + cleanfilename + datetime.now().strftime("%m%d%Y_%H%M") + ".txt"
        newFile = open(newFileName, "w+")
        newFile.write("Analysis of " + filebase)
        for q in quickfacts:
            newFile.write(q)
        #taggedCorpus = posAnalysis(sentences, cleanfilename) #if we want to do anything with the tagged corpus
        analysesList = ["Term Search", "Zipf Distribution", "Search for Group of Words", "Search for Sentences Containing Two Terms", "Plot Tagged Corpus"]#, "Collocution", "Concordance"] #put names of analyses in here 
        analysesVals = []
        analysesVals = easygui.multchoicebox("Select the analyses to be performed. ", "Selection", analysesList)
        #theFile = thisFile.read()
        #tokens = nltk.word_tokenize(theFile)
        if(analysesVals != None):
            for choice in analysesVals:
                if(choice == analysesList[0]):
                    newFile.write(TermSearch(sentences))
                if(choice == analysesList[1]):
                    ZipfDistribution(cleanfilename, top1000)
                if(choice == analysesList[2]):
                    newFile.write(groupSearch(sentences))
                if(choice == analysesList[3]):
                    newFile.write(doubleWordSearch(sentences))
                if(choice == analysesList[4]):
                    newFile.write(posAnalysis(sentences, cleanfilename))
##                if(choice == analysesList[4]):
##                    #newFile.write(collocation(words))
##                    collocation(thisFile)
##                #if(choice == analysesList[5]):
##                #    newFile.write(concordances(thisFile))
                    
        thisFile.close() #keep at end of main :)
        newFile.close() 

if __name__ == "__main__":
    while True: #Pretending to have a do-while loop
        main()
        runagain = easygui.ynbox("Choose another file?", "COCOA")
        if (runagain == False):
            break
